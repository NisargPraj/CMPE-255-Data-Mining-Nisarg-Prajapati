# Decision Tree and Ensemble Assignment

This repository contains several Colab notebooks implementing core machine learning algorithms from scratch and showcasing popular Gradient Boosting techniques. Below is an overview of each notebook:

---

## 1. Gradient Boosting Machine (GBM) from Scratch
This notebook demonstrates the implementation of the Gradient Boosting Machine (GBM) algorithm from scratch. It includes:
- Custom implementation of boosting with decision trees as weak learners.
- Training on a synthetic regression dataset.
- Evaluation using Mean Squared Error (MSE).

[Open GBM from Scratch Colab](https://colab.research.google.com/drive/1wzXCEk6LDQw4FUPv8cM6DEiyVAz4Q3go?usp=sharing)

---

## 2. Random Forest from Scratch
This notebook provides a step-by-step implementation of the Random Forest algorithm. It features:
- Bootstrapping of datasets and feature sampling.
- Building multiple decision trees as weak learners.
- Combining predictions via majority voting for classification.

[Open Random Forest from Scratch Colab](https://colab.research.google.com/drive/16kSERLi44HgghaA8RNbRC22d59oubog_?usp=sharing)

---

## 3. AdaBoost from Scratch
This notebook implements the AdaBoost algorithm from scratch. It includes:
- Weighted updates for misclassified samples.
- Training weak learners (decision stumps) iteratively.
- Evaluation on a synthetic classification dataset using accuracy.

[Open AdaBoost from Scratch Colab](https://colab.research.google.com/drive/1twP1uGUFZWuwMSdJazpuHkC0z80CHC07?usp=sharing)

---

## 4. Decision Trees from Scratch
This notebook features the implementation of a Decision Tree algorithm from scratch. It covers:
- Splitting criteria using Gini Index.
- Recursively building the tree until a max depth or minimum node size.
- Testing on the Iris dataset for classification tasks.

[Open Decision Trees from Scratch Colab](https://colab.research.google.com/drive/1ngiwmyTaPnDS7IXt2zgqRM6gmLSRIzfb?usp=sharing)

---

## 5. Gradient Boosting Techniques
This notebook showcases the usage of popular gradient boosting frameworks:
### a) Classifiers
- **XGBoost**, **LightGBM**, **CatBoost**, **Random Forest**, **AdaBoost**, and **Decision Trees** are trained and compared on a classification dataset.

[Open GBM Classifiers Colab](https://colab.research.google.com/drive/1z1XQvJu0SsdksMIOdaX9iEvRRYv5rpGA?usp=sharing)

### b) Regressors
- Gradient Boosting Regressors using **XGBoost**, **LightGBM**, and **CatBoost** on a regression dataset.

[Open GBM Regressors Colab](https://colab.research.google.com/drive/1o7gH2dJ4EUye3q9TK4ehs8MgOYW1fdEc?usp=sharing)

### c) Ranking Models
- Gradient Boosting Ranking techniques using **XGBoost** and **LightGBM** on a synthetic ranking dataset.

[Open GBM Ranking Colab](https://colab.research.google.com/drive/1JdHePYw19fl0RdERIy1-e2rNuokndAqB?usp=sharing)

---

